{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo 21: Ignacio de la Cruz Crespo y Sergio José Gómez Cortés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado A\n",
    "\n",
    "*Dividimos el conjunto de datos en 60% y 40% para train y test, respectivamente*\n",
    "Creamos una bolsa de palabras con:\n",
    "- binary = False\n",
    "- diccionario = words.txt\n",
    "- stop_words = 'english'\n",
    "- n_gramas = (1,1)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 11314\n",
      "Test texts: 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "train_data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "print(\"Training texts:\", len(train_data.data))\n",
    "print(\"Test texts:\", len(test_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "with open('words.txt') as f:\n",
    "    dictionary = f.read().splitlines()\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=dictionary, stop_words='english',ngram_range=(1,1), binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text                Type\n",
      "0                                                                        \n",
      "1   From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...         alt.atheism\n",
      "2   From: bil@okcforum.osrhe.edu (Bill Conner)\\nSu...         alt.atheism\n",
      "3   From: keith@cco.caltech.edu (Keith Allan Schne...         alt.atheism\n",
      "4   From: clldomps@cs.ruu.nl (Louis van Dompselaar...       comp.graphics\n",
      "..                                                ...                 ...\n",
      "56  From: dreitman@oregon.uoregon.edu (Daniel R. R...  talk.politics.misc\n",
      "57  From: wdstarr@athena.mit.edu (William December...  talk.politics.misc\n",
      "58  From: agr00@ccc.amdahl.com (Anthony G Rose)\\nS...  talk.religion.misc\n",
      "59  From: dotsonm@dmapub.dma.org (Mark Dotson)\\nSu...  talk.religion.misc\n",
      "60  From: sandvik@newton.apple.com (Kent Sandvik)\\...  talk.religion.misc\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Cojemos 3 mensajes de cada grupo y los metemos en un dataFrame\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "cat = fetch_20newsgroups().target_names\n",
    "tipo = np.array(\"\")\n",
    "text = np.array(\"\")\n",
    "for i in cat:\n",
    "    for j in range(4,7):\n",
    "        tipo = np.append(tipo,i)\n",
    "        text = np.append(text,fetch_20newsgroups(categories=[i]).data[j])\n",
    "threemessage = {'Text': text,\n",
    "        'Type': tipo\n",
    "        }\n",
    "df = pd.DataFrame(threemessage, columns = ['Text', 'Type'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ésta funcion devuelve los 3 mensajes del tema pasado por parametro\n",
    "def get3message(tipo):\n",
    "    select_col = df.loc[df['Type'] == tipo]\n",
    "    return select_col['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devuelve la precision media para la clase pasada en category\n",
    "def calculoPrecision(nivelExhaustividad,category, dataCosine):\n",
    "    m = np.array([])\n",
    "    for i in range(len(dataCosine)):\n",
    "        m = np.append(m,[np.mean(dataCosine[i])])  #Hago la media del resultado de cosine similarity para los 3 mensajes               \n",
    "    m = np.flip(np.argsort(m)) #'m' contiene las posiciones de train_data de menor a mayor distancia del coseno, asi que le damos la vuelta\n",
    "    #Este bucle mira las n = nivelExhaustividad primeras posiciones de m y las compara con la categoria que debe ser,\n",
    "    # para calcular el porcentaje de acierto\n",
    "    precision = 0\n",
    "    for j in range(nivelExhaustividad):\n",
    "        if (train_data.target_names[train_data.target[m[j]]] == category):\n",
    "            precision = precision + 1\n",
    "    return precision/nivelExhaustividad*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: alt.atheism\n",
      "Precision con nivel exhaustividad 10: 80.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.graphics\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.os.ms-windows.misc\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.sys.ibm.pc.hardware\n",
      "Precision con nivel exhaustividad 10: 40.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.sys.mac.hardware\n",
      "Precision con nivel exhaustividad 10: 80.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.windows.x\n",
      "Precision con nivel exhaustividad 10: 40.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: misc.forsale\n",
      "Precision con nivel exhaustividad 10: 50.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.autos\n",
      "Precision con nivel exhaustividad 10: 50.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.motorcycles\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.sport.baseball\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.sport.hockey\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.crypt\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.electronics\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.med\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.space\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: soc.religion.christian\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.guns\n",
      "Precision con nivel exhaustividad 10: 30.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.mideast\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.misc\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.religion.misc\n",
      "Precision con nivel exhaustividad 10: 40.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "train_vector_data = vectorizer.transform(train_data.data) #Vectorizamos train data\n",
    "for i in range (len(fetch_20newsgroups().target_names)):\n",
    "    #Calculamos la distancia del coseno entre train_vector_data y get3message(target_names), es decir, la distancia\n",
    "    #del coseno entre los mensajes de entrenamiento y los de consulta\n",
    "    test = cosine_similarity(train_vector_data, vectorizer.transform(get3message(fetch_20newsgroups().target_names[i])))\n",
    "    print(\"Categoria: \" + fetch_20newsgroups().target_names[i])\n",
    "    print(\"Precision con nivel exhaustividad 10: \"+str(calculoPrecision(10,fetch_20newsgroups().target_names[i],test)))\n",
    "    print(\"Precision con nivel exhaustividad 3: \"+str(calculoPrecision(3,fetch_20newsgroups().target_names[i],test)))\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Hay muchas diferencias entre los valores de precisión medios para las distintas clases del conjunto de datos? ¿A qué crees que se deben?**\n",
    "\n",
    "Podemos encontrar una gran diferencia entre algunos topicos, notablemente entre motorcycles y politics.guns, variando de un 100% a un 30% respectivamente. Esto puede ser debido a la existencia de palabras clave en el mundo de las motocicletas que no se comparten fuera de ese ambito. Del mismo modo encontramos una notable igualdad entre todas los calculos con exhaustividad 3, que se deben a la poca piscina de muestras con las que se han realizado, y por lo tanto no se pueden considerar representativas.\n",
    "\n",
    "---\n",
    "*A continuación identificamos la clase que haya tenido peores resultados de precisión y mostramos algun mensaje que recuperó erróneamente en las primeras X posiciones.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar a calculo precision, hacemos un print de los mensajes que han sido erroneos dentro de las nivelExhaustividad primeras\n",
    "#posiciones\n",
    "def mensajesErroneos(nivelExhaustividad,category, dataCosine):\n",
    "    m = np.array([])\n",
    "    for i in range(len(dataCosine)):\n",
    "        m = np.append(m,[np.mean(dataCosine[i])])                 \n",
    "        #print(m)\n",
    "    m = np.flip(np.argsort(m))\n",
    "    for j in range(nivelExhaustividad):\n",
    "        if (train_data.target_names[train_data.target[m[j]]] != category):\n",
    "            print(train_data.data[m[j]])\n",
    "            print(train_data.target_names[train_data.target[m[j]]])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: talk.politics.guns\n",
      "From: matmcinn@nuscc.nus.sg (Matthew MacIntyre at the National University of Senegal)\n",
      "Subject: Re: Opel owners?\n",
      "Organization: National University of Singapore\n",
      "Lines: 1\n",
      "X-Newsreader: Tin 1.1 PL4\n",
      "\n",
      "By the way, what do people think about the Opel CAlibra?\n",
      "\n",
      "rec.autos\n",
      "From: ee152fcs@sdcc15.ucsd.edu (Bjorn Karlsson)\n",
      "Subject: Re: WC 93: Results, April 20\n",
      "Organization: University of California, San Diego\n",
      "Lines: 12\n",
      "Nntp-Posting-Host: sdcc15.ucsd.edu\n",
      "\n",
      "In article <1993Apr21.073134.5117@ericsson.se> etxonss@ufsa.ericsson.se (Staffan Axelsson) writes:\n",
      ">\n",
      "> 1993 World Championships in Germany:\n",
      "> ====================================\n",
      "\n",
      "Is there any games being shown here in the US from the WC???\n",
      "\n",
      "Thanks\n",
      "\n",
      "mc\n",
      "\n",
      "\n",
      "\n",
      "rec.sport.hockey\n",
      "From: hungjenc@usc.edu (Hung-Jen Chen)\n",
      "Subject: test\n",
      "Article-I.D.: phakt.1pqgltINN9dg\n",
      "Distribution: na\n",
      "Organization: University of Southern California, Los Angeles, CA\n",
      "                test\n",
      "Lines: 6\n",
      "NNTP-Posting-Host: phakt.usc.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      \n",
      "\n",
      "misc.forsale\n",
      "From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau)\n",
      "Subject: Re: Biblical Rape\n",
      "Organization: Technical University Braunschweig, Germany\n",
      "Lines: 14\n",
      "\n",
      "In article <1993Apr05.174537.14962@watson.ibm.com>\n",
      "strom@Watson.Ibm.Com (Rob Strom) writes:\n",
      " \n",
      ">\n",
      ">In article <16BA7F16C.I3150101@dbstu1.rz.tu-bs.de>, I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau) writes:\n",
      ">\n",
      ">I didn't have time to read the rest of the posting, but\n",
      ">I had to respond to this.\n",
      ">\n",
      ">I am absolutely NOT a \"Messianic Jew\".\n",
      ">\n",
      " \n",
      "Another mistake. Sorry, I should have read alt.,messianic more carefully.\n",
      "   Benedikt\n",
      "\n",
      "alt.atheism\n",
      "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
      "Subject: Re: New Member\n",
      "Organization: Salvation Army Draft Board\n",
      "Lines: 28\n",
      "\n",
      "In article <C5HIEw.7s1@portal.hq.videocart.com> dfuller@portal.hq.videocart.com (Dave Fuller) writes:\n",
      ">\n",
      ">  Hello. I just started reading this group today, and I think I am going\n",
      ">to be a large participant in its daily postings. I liked the section of\n",
      ">the FAQ about constructing logical arguments - well done. I am an atheist,\n",
      ">but I do not try to turn other people into atheists. I only try to figure\n",
      ">why people believe the way they do - I don't much care if they have a \n",
      ">different view than I do. When it comes down to it . . . I could be wrong.\n",
      ">I am willing to admit the possibility - something religious followers \n",
      ">dont seem to have the capability to do.\n",
      ">\n",
      ">  Happy to be aboard !\n",
      ">\n",
      ">Dave Fuller\n",
      ">dfuller@portal.hq.videocart.com\n",
      "\n",
      "Welcome.  I am the official keeper of the list of nicknames that people\n",
      "are known by on alt.atheism (didn't know we had such a list, did you).\n",
      "Your have been awarded the nickname of \"Buckminster.\"  So the next time\n",
      "you post an article, sign with your nickname like so:\n",
      "Dave \"Buckminster\" Fuller.  Thanks again.\n",
      "\n",
      "Jim \"Humor means never having to say you're sorry\" Copeland\n",
      "--\n",
      "If God is dead and the actor plays his part                    | -- Sting,\n",
      "His words of fear will find their way to a place in your heart | History\n",
      "Without the voice of reason every faith is its own curse       | Will Teach Us\n",
      "Without freedom from the past things can only get worse        | Nothing\n",
      "\n",
      "alt.atheism\n",
      "From: anderge@stein.u.washington.edu (Geoff Anderson)\n",
      "Subject: Re: Fenway Gif\n",
      "Organization: University of Washington, Seattle\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: stein.u.washington.edu\n",
      "\n",
      "In article <C5JB3D.9nt@umassd.edu> acsddc@smucs1.umassd.edu writes:\n",
      ">I was wondering if anyone had any kind of Fenway Park gif.\n",
      ">I would appreciate it if someone could send me one.\n",
      ">Thanks in advance.\n",
      ">\n",
      ">-Dan\n",
      "\n",
      "Me too!  I would like any park or action gif or jpeg about baseball.\n",
      "\n",
      "Geoff\n",
      "\n",
      "\n",
      "rec.sport.baseball\n",
      "From: spinoza@next06wor.wam.umd.edu (Yon Bonnie Laird of Cairn Robbing)\n",
      "Subject: Re: ISLAM BORDERS. ( was :Israel: misisipi to ganges)\n",
      "Nntp-Posting-Host: next03wor.wam.umd.edu\n",
      "Organization: Workstations at Maryland, University of Maryland, College Park\n",
      "Lines: 20\n",
      "\n",
      "In article <1993Apr5.183555.20163@thunder.mcrcim.mcgill.edu>  \n",
      "hasan@McRCIM.McGill.EDU writes:\n",
      "> \n",
      "> In article <4805@bimacs.BITNET>, ehrlich@bimacs.BITNET (Gideon Ehrlich)  \n",
      "writes:\n",
      "> |> \n",
      "> |> Hassan and some other seemed not to be a ware that Jews celebrating  \n",
      "on\n",
      "> |> these days Thje Passover holliday the holidy of going a way from the\n",
      "> |> Nile.\n",
      "> |> So if one let his imagination freely work it seemed beter to write\n",
      "> |> that the Zionist drean is \"from the misisipi to the Nile \".\n",
      "> \n",
      "> the question is by going East or West from the misisipi. on either  \n",
      "choice\n",
      "> you would loose Palestine or Broklyn, N.Y.\n",
      "> \n",
      "> I thought you're gonna say fromn misisipi back to the misisipi !\n",
      "> \n",
      "Nonononnononono....its \"From the Nile to the Nile.....the Long way!\" ;-)\n",
      "\n",
      "talk.politics.mideast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #talk.politics.guns es la clase con peor resultado (30%) como se ve previamente\n",
    "    test2 = cosine_similarity(train_vector_data, vectorizer.fit_transform(get3message(\"talk.politics.guns\")))\n",
    "    print(\"Categoria: \" + \"talk.politics.guns\")\n",
    "    mensajesErroneos(10,\"talk.politics.guns\",test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Con qué clases se ha confundido más dicha consulta?**\n",
    "\n",
    "Como se ve en la celda de arriba estas son las clases con las que mas se confunde puesto que son las clases de los mensajes con mayor similitud de coseno y que no son de la clase \n",
    "- talk.politics.guns\n",
    "- rec.autos\n",
    "- rec.sport.hockey\n",
    "- misc.forsale\n",
    "- alt.atheism\n",
    "- alt.atheism\n",
    "- rec.sport.baseball\n",
    "- talk.politics.mideast\n",
    "\n",
    "**¿A qué crees que se deben los malos resultados?**\n",
    "\n",
    "A la utilizacion de una gran variedad de palabras que se puedan relacionar con varias categorias y al solapamiento de estas, por ejemplo entre política, armas y religión (atheism), en los que se tratan topicos con un léxico similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado B\n",
    "\n",
    "- Usamos un TF-IDF para ponderar el peso de los terminos de la bolsa de palabras\n",
    "- Dicho TF-IDF lo transformamos con countVectorizer(binary = False) y aplicamos TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerB = CountVectorizer(vocabulary=dictionary, stop_words='english',ngram_range=(1,1), binary=False)  \n",
    "tfidfer = TfidfTransformer()\n",
    "\n",
    "vectorDataB = vectorizerB.fit_transform(train_data.data)\n",
    "vectorMod = tfidfer.fit_transform(vectorDataB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: alt.atheism\n",
      "Precision con nivel exhaustividad 10: 80.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.graphics\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.os.ms-windows.misc\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.sys.ibm.pc.hardware\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.sys.mac.hardware\n",
      "Precision con nivel exhaustividad 10: 80.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: comp.windows.x\n",
      "Precision con nivel exhaustividad 10: 50.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: misc.forsale\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.autos\n",
      "Precision con nivel exhaustividad 10: 50.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.motorcycles\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.sport.baseball\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: rec.sport.hockey\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.crypt\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.electronics\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.med\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: sci.space\n",
      "Precision con nivel exhaustividad 10: 100.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: soc.religion.christian\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.guns\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.mideast\n",
      "Precision con nivel exhaustividad 10: 90.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.politics.misc\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n",
      "Categoria: talk.religion.misc\n",
      "Precision con nivel exhaustividad 10: 70.0\n",
      "Precision con nivel exhaustividad 3: 100.0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "#Calculamos la precision media como en el apartado anterior pero en este caso en la bolsa de palabras hemos modulado \n",
    "#la frecuencia segun lo popular que es el termino en train\n",
    "for i in range (len(fetch_20newsgroups().target_names)):\n",
    "    #Calculamos la distancia del coseno entre train_vector_data y get3message(target_names), es decir, la distancia\n",
    "    #del coseno entre los mensajes de entrenamiento y los de consulta\n",
    "    test = cosine_similarity(vectorMod, vectorizer.transform(get3message(fetch_20newsgroups().target_names[i])))\n",
    "    print(\"Categoria: \" + fetch_20newsgroups().target_names[i])\n",
    "    print(\"Precision con nivel exhaustividad 10: \"+str(calculoPrecision(10,fetch_20newsgroups().target_names[i],test)))\n",
    "    print(\"Precision con nivel exhaustividad 3: \"+str(calculoPrecision(3,fetch_20newsgroups().target_names[i],test)))\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Han cambiado los valores de precisión media para las clases del conjunto de datos? ¿Qué clases han mejorado? ¿Cuáles han empeorado?**\n",
    "\n",
    "Han cambiado notablemente los valores de precisión media, en concreto han mejorado: \n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.windows.x\n",
    "- misc.forsale\n",
    "- rec.sport.hockey\n",
    "- sci.crypt\n",
    "- soc.religion.christian\n",
    "- talk.politics.guns (notablemente)\n",
    "- talk.religion.misc\n",
    "Han empeorado:\n",
    "- talk.politics.mideast\n",
    "\n",
    "---\n",
    "\n",
    "La consulta en talk.politics.guns ha mejorado de un 30% a un 70%, esto es debido a que TF/IDF elimina palabras de uso comun no ligadas al contexto de la categoria en la que se encuentre, esto implica que sea mas facil diferenciar categorías con léxicos similares, como le ocurre a guns con otras categorias como por ejemplo \"atheism\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
